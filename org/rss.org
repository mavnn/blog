#+TITLE: Mavnn's blog

* Log in, log out: Dev Journal 4 (part 2)
:PROPERTIES:
:RSS_PERMALINK: 2024/03/05/dev_journal_4_2.html
:PUBDATE: 2024-03-05
:ID:       DD7DD12C-3A30-416F-A16B-8C0568CA634E
:END:
#+begin_quote
This post is the second half of a two part update in the "Dev Journal" series. [[file:../../../2024/03/01/dev_journal_4.org][The first half]] talks about adding dependencies to the project on postgresql and the Marten event store library, which we'll look at using in this post. [[file:../../../2024/01/31/dev-journal-1.org][Part 1]] contains the series index, while the [[https://gitlab.com/mavnn/caldance/-/commits/DevJournal4?ref_type=tags][DevJournal4]] tag for the CalDance project in GitLab holds the state of the repository as described here.
#+end_quote

So. We have an event store. Our website is going to have users. How do we go about user management?

** Where's the cheese?
:PROPERTIES:
:ID:       F90DDBF3-704B-4BBA-BA24-E00EE4477964
:END:

To borrow a term from domain driven design, this sounds like a "bounded context" within our system. Other parts of the code may care about certain events happening related to users (users being created, that kind of thing), but they probably shouldn't know or care about how the internals of "a user" work or what it takes to authenticate a user.

There are as many ways of organizing your code as there are grains of sand on the beach, but fundamentally all of the ones that help are about choosing where to have boundaries in your code base.

We are going to have three horizontal slices; shared library code, domain logic (our "business" code), and execution environment. Vertically we're going to slice the domain logic by bounded context - of which, admittedly, we only have one at the moment.

We end up with something like (things further down the table depend on the things above):

+--------------------------------------------------+
| Http Handler abstraction, UI components          |
+-------------------+------------------------------+
| User domain logic | Things users do domain logic |
+-------------------+------------------------------+
| Read configuration files, start the web server   |
+--------------------------------------------------+

You'll notice that this doesn't group the code by the technical task the code is trying to achieve, a pattern you'll often find in example project templates where you'll end up with a "Controllers" directory and a "Views" directory. It's also not an organization along "clean/hexagon/ports and adapters" lines with a strict demarcation between code that speaks to the outside world achieved via interfaces and abstractions.

It's not that I feel that either of those patterns has no merit (although I feel like the main driver of the first pattern is that you can suggest it even for projects you /know nothing about/ which is a useful property when writing templates and dispensing nuggets of wisdom at conferences about the *one true way* to organize code). But I do feel that for the vast majority of code bases, it is a far bigger gain to productivity to be able to co-locate code by /purpose/ than by /type/.

Let's face it: while you sometimes pick up a story/card/work ticket that requires you to go and change all the controllers (normally during dependency upgrades), or replace all the database interface implementations (you're about to have a long few months), it is much more likely on a day to day basis that you're trying to add a new field to the data we store about users, and you want to update the data store, business logic, and UI of /users/ to be able to do that. Taking this logic to its logical extremes leads you towards microservices - but that starts to bring in a different type of complexity of its own.

All of this to say: there's now a folder called ~Domain~ which holds our new, shiny, user management code in a file called: /drumroll, please/ ~User.fs~. Let's have a look at it in detail.

** The cheese. We have found it.
:PROPERTIES:
:ID:       05D577B8-A347-484F-BEF3-6D93C82FB8E1
:END:

#+begin_src fsharp
  module Mavnn.CalDance.Domain.User

  open Falco
  open Falco.Routing
  open Falco.Markup
  open Falco.Security
  open Marten
  open Marten.Events.Aggregation
  open Marten.Events.Projections
  open Mavnn.CalDance
  open Mavnn.CalDance.Routing
  open System.Security.Claims
  open Microsoft.AspNetCore.Identity
#+end_src

As just mentioned, this module is going to be responsible for the whole vertical slice of the application for user management, so we start by including everything we need from the data store (~Marten~) through to the UI (~Falco.Markup~). We could have created sub modules within a Users folder if needed, but the module is only ~300 lines long so I haven't split it up (yet).

#+begin_src fsharp
  type User = { id: System.Guid; username: string }

  type UserState =
    | Active
    | Disabled

  [<CLIMutable>]
  type UserRecord =
    { Id: System.Guid
      Username: string
      PasswordHash: string
      State: UserState }

  type UserEvent =
    | Created of UserRecord
    | PasswordChanged of passwordHash: string
    | Disabled
#+end_src

Next we define a few data types that represent our users, and the events that can happen to them over time. This is important because we are "event sourcing" the state of our users, meaning that the golden source of truth for what state a user is in is defined by what events have happened to them so far. The two representations of the user represent what we care about in the running system (the main ~User~ type) and what we need to store about them on disk (the ~UserRecord~ type); in general we would expect that other modules /might/ make use of the ~User~ type but in general they should not make use of the ~UserRecord~ type. Its an open question in my mind whether it should actually be marked as a private type declaration, but I've erred on the side of leaving it available for now.

A minor implementation detail: to try and keep the incremental steps of the project manageable I'm using the default (de)serializers for Marten, which require the object to be deserialized from the data base has a default constructor and mutable fields, which we get from the ~[<CLIMutable>]~ attribute. We'll probably remove that going forwards by switching to a serialization strategy that works with immutable F# records.

The life cycle of our users is very simple at the moment; a ~Created~ event signals that a new, active, user was created. That user can change their password, or they can be marked disabled which effectively ends the lifecycle of the user. There's no way to reactivate a user now, although we could always add one later.

#+begin_src fsharp
  type UserRecordProjection() =
    inherit SingleStreamProjection<UserRecord>()

    member _.Create(userEvent, metadata: Events.IEvent) =
      match userEvent with
      | Created user -> user
      | _ ->
        // We should always receive a created event
        // first so this shouldn't ever happen...
        // ...but it might, and we don't want to throw
        // in projections.
        { Id = metadata.Id
          Username = ""
          PasswordHash = ""
          State = UserState.Disabled }


    member _.Apply(userEvent, userRecord: UserRecord) =
      task {

        match userEvent with
        | Created _ ->
          // Should never occur after the first event in the stream
          // so we ignore duplicates
          return userRecord
        | PasswordChanged passwordHash ->
          match userRecord with
          | { State = UserState.Disabled } ->
            // Don't update password of disabled users
            return userRecord
          | user ->
            return
              { user with
                  PasswordHash = passwordHash }
        | Disabled ->
          match userRecord with
          | { State = UserState.Disabled } ->
            return userRecord
          | { State = Active } ->
            return
              { userRecord with
                  State = UserState.Disabled }
      }
#+end_src

~Marten~ leans heavily into the code reflection capabilities of the dotnet framework, allowing us to configure our data store in terms of the in program types we want it to store. A "projection" in event sourcing is the logic which takes a list of events (our base line source of truth) and turns it into a current state, so this class defines a projection that will create and/or update ~UserRecord~ data in Marten's document store (we know it does this because it implements the ~SingleStreamProjection<UserRecord>~ interface). It will project /from/ events of the ~UserEvent~ type, because that is the type of the first argument of the ~Create~ and ~Apply~ methods we have supplied.

There are a few conventions we need to follow here to allow for this minimalist a configuration. Our current state type /must/ have an ~Id~ (or ~id~) field of type string, uuid, or integer. And when an event matching the signature of our projection is pushed to a stream with an ID, the resulting update to the current status type must produce a document with the same ID as the stream ID.

We're treating our records as immutable objects (because we're planning to make them immutable going forward), so our create and apply methods return a ~Task<UserRecord>~; if the document type was mutable we would also have the options of mutating it in place and returning void.

With that explanation out of the way, hopefully the state machine that represents our user life cycle is clear in the code above.

Now that we can store information about our users, and update them based on what is happening to them, it's time to start implementing the actual responsibilities of the module. We're keeping things minimal to get started, so we'll implement only the three things we /really/ need: sign up, log in, and log out.

#+begin_src fsharp
  type LoginFormData = { username: string; password: string }

  let findUserRecord (username: string) =
    Marten.withMarten (fun marten ->
      marten
        .Query<UserRecord>()
        .SingleOrDefaultAsync(fun ur ->
          ur.Username = username))
    |> Handler.map Marten.returnOption

  let loginRoute = RouteDef.literalSection "/login"
  let logoutRoute = RouteDef.literalSection "/logout"
  let signupRoute = RouteDef.literalSection "/signup"

  let getSessionUser: Handler<User option> =
    Handler.fromCtx (fun ctx ->
      match ctx.User with
      | null -> None
      | principal ->
        match
          (System.Guid.TryParse(
            principal.FindFirstValue("userId")
           ),
           principal.FindFirstValue("name"))
        with
        | ((false, _), _)
        | (_, null) -> None
        | ((true, id), username) ->
          Some { id = id; username = username })
#+end_src

A few definitions and helpers start us off; what data a form needs to capture for someone to sign up/log on, what urls exist and are managed by this module, and a couple of helper functions for obtaining a user record and a user session from the current HTTP context (using the ~Handler~ type we talked about in the last post).

#+begin_src fsharp
  let loginGetEndpoint =
    Handler.toEndpoint get loginRoute (fun () ->
      Handler.return' (
        Response.ofHtmlCsrf (fun csrfToken ->
          Elem.html
            []
            [ Elem.body
                []
                [ Elem.form
                    [ Attr.method "post" ]
                    [ Elem.input [ Attr.name "username" ]
                      Elem.input [ Attr.name "password" ]
                      Xss.antiforgeryInput csrfToken
                      Elem.input
                        [ Attr.type' "submit"
                          Attr.value "Submit" ] ] ] ])
      ))
#+end_src

Our first end point is straight forward. When we receive a get request to the login path, we reply with a form containing a token to prevent cross site vulnerabilities and username and password fields.

#+begin_src fsharp
  let private makePrincipal userRecord =
    let claims =
      [ new Claim("name", userRecord.Username)
        new Claim("userId", userRecord.Id.ToString()) ]

    let identity = new ClaimsIdentity(claims, "Cookies")

    new ClaimsPrincipal(identity)

  let passwordHasher = PasswordHasher()

  let updateUser (id: System.Guid, events: seq<UserEvent>) =
    handler {
      do!
        Marten.withMarten (fun marten ->
          task {
            // explicitly assign this as an array of objects
            // so that Marten chooses the correct method
            // overload for `Append`
            let eventObjs: obj[] =
              Array.ofSeq events |> Array.map box

            marten.Events.Append(id, eventObjs) |> ignore
            return! marten.SaveChangesAsync()
          })

      return!
        Marten.withMarten (fun marten ->
          marten.LoadAsync<UserRecord>(id))
    }
#+end_src

Our next end point is going to actually handle the form coming in, so it requires a few more helpers. The web framework we're using will handle things like sessions for us, but only if we "buy into" the .NET standard ways of representing a user, in this case using the ~ClaimsPrincipal~ type - so we have a helper to map from one of our user records to a claims principal. We initialize a password hasher which will salt and hash our passwords for us (don't roll your own crypto, folks, especially when your language ecosystem has a decent implementation ready for you). And finally we add an other method that works within our HTTP context expressions - ~updateUser~ takes the ID of a user and a list of events and returns the updated ~UserRecord~.

With all of that in place, we can write the ~loginPostEndpoint~.

#+begin_src fsharp
  let loginPostEndpoint =
    Handler.toEndpoint post loginRoute (fun () ->
      handler {
        let! loginData =
          Handler.formDataOrFail
            (Response.withStatusCode 400 >> Response.ofEmpty)
            (fun f ->
              Option.map2
                (fun username password ->
                  { username = username
                    password = password })
                (f.TryGetStringNonEmpty "username")
                (f.TryGetStringNonEmpty "password"))

        let! userRecord =
          findUserRecord loginData.username
          |> Handler.ofOption (
            Response.withStatusCode 403 >> Response.ofEmpty
          )

        let verificationResult =
          passwordHasher.VerifyHashedPassword(
            userRecord,
            userRecord.PasswordHash,
            loginData.password
          )

        match verificationResult with
        | PasswordVerificationResult.Failed ->
          return
            (Response.withStatusCode 403 >> Response.ofEmpty)
        | PasswordVerificationResult.Success ->
          return
            Response.signInAndRedirect
              "Cookies"
              (makePrincipal userRecord)
              "/"
        | PasswordVerificationResult.SuccessRehashNeeded ->
          let! _ =
            updateUser (
              userRecord.Id,
              [ PasswordChanged(
                  passwordHasher.HashPassword(
                    userRecord,
                    loginData.password
                  )
                ) ]
            )

          return
            Response.signInAndRedirect
              "Cookies"
              (makePrincipal userRecord)
              "/"
        | _ ->
          return
            failwithf
              "Unknown password verification result type %O"
              verificationResult

      })
#+end_src

Time to actually use our ~handler~ expression in earnest! There is some personal preference in play here, but personally I really like the clear flow of the request we can see happening in this code. We either have the form data we need, or we return a ~400~ error. Then we either find a user record with a matching username, or we return a ~403~ error (we don't want to reveal whether a username exists or not, so we return the same code as for when the password is incorrect; security +1, helpful error messages to users -1). Then we check the password, and we either return ~403~ (if it is wrong) or log you in if it is correct. A minor piece of extra complexity is introduced by the fact that the password hasher may signal that the password is correct but the /hash/ needs updating in storage, a background operation that the user does not need to know about.

I'll leave the other end points for the reader to read at their leisure [[https://gitlab.com/mavnn/caldance/-/blob/e62126228d63e77834112a193fcb0396f4410bc5/Server/src/Domain/User.fs][on Gitlab]], as they are either trivial (~logoutEndpoint~) or very similar to the log in end points (~signupGetEndpoint~ and ~signupPostEndpoint~).

Finally, we get to the end of the module where we export everything that the web server setup code (the bottom layer in my newly christened "julienned domain sandwich" architecture).

#+begin_src fsharp
  let endpoints =
    [ loginGetEndpoint
      loginPostEndpoint
      logoutEndpoint
      signupGetEndpoint
      signupPostEndpoint ]

  let martenConfig (storeOptions: Marten.StoreOptions) =
    storeOptions.Projections.Add<UserRecordProjection>(
      ProjectionLifecycle.Inline
    )
#+end_src

At the moment, with only one domain, this is just an adhoc export of the end points we're wanting to add to the webserver and the projections we want to add to ~Marten~. As the project grows, we'll probably add an interface that each of our domain modules will export which will provide to allow a standardized process for consuming the needed configuration. But there's little point trying to proactively create an abstraction over a single example of a pattern.

And there you have it; event sourced (basic) user management for our web application. If you have thoughts and questions, drop them as an issue on the [[https://gitlab.com/mavnn/caldance/-/blob/e62126228d63e77834112a193fcb0396f4410bc5/Server/src/Domain/User.fs][CalDance repository]]. I'd love to see example repositories having in depth discussions of when the architecture they suggest is or isn't useful, even if (especially if!) that discussion includes comments critical of the architecture demonstrated.

Next up: who knows? But probably a bit of testing and refactoring, our code is already a little messy in a few places.
* Foundations: Dev Journal 1
:PROPERTIES:
:RSS_PERMALINK: 2024/01/31/dev-journal-1.html
:PUBDATE: 2024-01-31
:ID:       5CC40DFE-A976-4C7D-8D04-B6AD83F4E269
:END:
This is something a little bit new. A series I'm starting that documents the building of a simple project from the ground up using a set of tools and techniques I've come to either really like, or that I'd like to try out.

On the one hand this is a personal project. On the other, I'd like to take advantage of nice things like CI/CD, testing, etc, even when I'm working on something for myself. So this is also a mini-tour of many of the things I would do setting up a new greenfield project for a team.

As the series progresses, I'll carry on adding the sections here.

*The series so far*

** [[https://blog.mavnn.co.uk/2024/01/31/dev-journal-1.html][Foundations]]: Build and package
:PROPERTIES:
:ID:       D556A63D-DBFD-4FE4-B147-0F95A3067703
:END:
** [[file:../../../2024/02/06/dev-journal-2.org][Scaffolding]]: Testing and consistency
:PROPERTIES:
:ID:       0653CD29-45F4-4498-AB72-B0F8BA789F94
:END:
** [[file:../../../2024/02/20/dev-journal-3.org][Does it run?]]: Make sure the docker container is valid and stays valid
:PROPERTIES:
:ID:       DFAFC2D1-3409-4C26-8434-BC45A1EB2FF2
:END:
** [[file:../../../2024/03/01/dev_journal_4.org][Log in, log out]] (and [[file:../../../2024/03/05/dev_journal_4_2.org][part 2]]): Adding the database and the ability to log into our web site
:PROPERTIES:
:ID:       ECCC61E0-8611-4FC9-89FE-3E0220EC69B1
:END:

** Part 1: Foundations
:PROPERTIES:
:ID:       2DFFD641-2C4B-42C0-A772-1BBFA9C86945
:END:

Our application will eventually be a little web site for ~redacted in case I change my mind~. I'm going to be using mix of tried and new tech (for me personally).

On the things I'd like to try front, we have:

** [[https://htmx.org/][htmx]] (probably with [[https://bulma.io/][bulma]] for initial styling) to provide the UI. This isn't going to be hugely interactive application, it is mostly going to collect information from forms, and display nice looking output tables so htmx's server side rendering model seems a perfect fit. I've used server side rendering in other projects and liked it, and htmx seems a low impact way to take that to the next level.
:PROPERTIES:
:ID:       B3A34548-5024-48E1-A6D7-D4E7EBBA0CDC
:END:
** [[https://www.falcoframework.com/][falco]] for writing the backend server in F#. [[https://xyncro.github.io/sites-freya.io/][Freya]], my webserver of choice for F# back in the day, is no longer actively maintained but it looks like Falco has taken some of its nicer features and done its own thing with them.
:PROPERTIES:
:ID:       5410F358-7CB0-4CC6-9DDE-126D4F1B7E1B
:END:

On the technologies I've used before and found useful front, we have:

** [[https://nixos.org/][nix]] to give a version controlled build/development environments and reproducible packaging.
:PROPERTIES:
:ID:       CE83A447-7BB6-429F-B112-0E8FDB666BB0
:END:
** [[https://direnv.net/][direnv]] for seamless local development environments.
:PROPERTIES:
:ID:       057A0B11-B786-4D8F-9D05-9B4D148789D7
:END:
** [[https://github.com/JasperFx/marten][marten]] from the "Critter Stack" as an event store on top of postgresql to build our datastore.
:PROPERTIES:
:ID:       205021B4-9549-4F88-86AC-7C8CAFB17C9D
:END:
** [[https://gitlab.com/][gitlab]] for code repository, container registry and CI/CD pipeline.
:PROPERTIES:
:ID:       54F711EA-F657-4145-9BA3-B0ABB1562F42
:END:

I'm not sure how far I'm going to take this experiment publicly, but what I'm going to focus on first is just the basics of any online app: people being able to sign up, log in, and manage an account for a paid service. At least that far the whole project will be MIT licensed, so if you like what you see you can just pick it up and use it as a starter template for your own project.

For today, let's start with a /minimum deployable product/: a "Hello world" Falco server with CI/CD pipeline in place. We'll have a gitlab hosted project anybody with a working nix environment can pull down and:

** run ~nix run~ and have a webserver running locally that will respond to get requests to ~/~ with "Hello world"
:PROPERTIES:
:ID:       2CA7146F-2471-40F3-9C42-65BBD77B4A49
:END:
** run ~nix build .#dockerImage~ to build a docker image with the same architecture they're using (i.e. ~aarch64-darwin~ if you run it on a Mac)
:PROPERTIES:
:ID:       E0BC04D5-EFA2-494D-9EFA-5344FC1782E6
:END:
** by pushing a commit to gitlab trigger a CI pipeline building said docker image for ~x86_64-linux~ and pushing it to a package registry ready to deploy
:PROPERTIES:
:ID:       226B5A4E-1D4E-4235-A25A-FB3D527D7176
:END:

Enough bullet points. What did I actually do? (Sneak preview: [[https://gitlab.com/mavnn/caldance/-/tree/6b39d13d98199220d623870faf2b49fbda58d8a5][browse the gitlab repo at the time of the commit that this post describes]])

*** Setup a nix flake to provide our environment
:PROPERTIES:
:ID:       633BD854-78FF-4350-952B-07C92D240A24
:END:

A nix "flake" is a declarative description of a set of packages we'd like to be able to reference. You can read the [[https://gitlab.com/mavnn/caldance/-/blob/6b39d13d98199220d623870faf2b49fbda58d8a5/flake.nix][whole file]] but the important part for today is that our ~flake.nix~ file specifies three outputs in this stanza:

#+begin_src nix
  # Tools we want available during development
  devShells.default = pkgs.mkShell {
    buildInputs = [ dnc.sdk_8_0 pkgs.nixfmt pkgs.skopeo ];
  };

  # Default result of running `nix build` with this
  # flake; it builds the F# project `CalDance.fsproj`
  packages.default = pkgs.buildDotnetModule {
    pname = name;
    version = "0.1";

    src = ./.;
    projectFile = "CalDance.fsproj";
    nugetDeps = nugets;

    # We set nix to create an output that contains
    # everything needed, rather than depending
    # on the dotnet runtime
    selfContainedBuild = true;

    # This is a webserver, and it complains if it
    # has no access to openssl
    runtimeDeps = [ pkgs.openssl pkgs.cacert ];

    dotnet-sdk = dnc.sdk_8_0;
    dotnet-runtime = dnc.runtime_8_0;
    executables = [ "CalDance" ];
  };

  # A target that builds a fully self-contained docker
  # file with the project above
  packages.dockerImage = pkgs.dockerTools.buildImage {
    name = name;
    config = {
      # asp.net likes a writable /tmp directory
      Cmd = pkgs.writeShellScript "runServer" ''
        ${pkgs.coreutils}/bin/mkdir -p /tmp
        ${pkgs.coreutils}/bin/mount -t tmpfs tmp /tmp
        ${packages.default}/bin/CalDance.Server
      '';
      Env =
        [ "DOTNET_EnableDiagnostics=0" "ASPNETCORE_URLS=http://+:5001" ];
      ExposedPorts = { "5001/tcp" = { }; };
    };
  };
#+end_src

First we say we want a shell environment which includes the dotnet core SDK (version 8), nixfmt (for formatting nix files), and skopeo which we can use for moving docker images around.

Then we define the default output for this flake: it uses the ~buildDotnetModule~ to specify that in our case it should build the executable ~CalDance~ based on the F# project file ~CalDance.fsproj~. A helper makes sure that Nix is aware of which nuget packages the project has referenced, so that they can be packaged correctly.

Finally, we define the ~dockerImage~ which uses the ~dockerTools.buildImage~ helper to say we want to be able to build a docker image that contains the executable from the default package above, everything it needs to run and /nothing else at all/. In our case, this produces a docker image weighing in at around 80MB - similar to what you'd get optimising a [[https://blogit.create.pt/telmorodrigues/2022/03/08/smaller-net-6-docker-images/][two step hand crafted dockerfile]], and significantly smaller than using the official [[https://hub.docker.com/_/microsoft-dotnet-aspnet/][Microsoft ASP.NET runtime image]].

*** direnv
:PROPERTIES:
:ID:       38C84D92-84B4-4F4B-A88F-A5C99EF9C7A1
:END:

Direnv is a tool that can add environment variables to your shell when you enter a directory. It also, conveniently, knows about Nix flakes.

We add a ~.envrc~ file to our project with the contents:

#+begin_src bash
  #!/usr/bin/env bash
  # the shebang is ignored, but nice for editors
  use flake
#+end_src

Next time we move into this directory, direnv will ask us to allow this ~.envrc~ file. If we accept, our normal local shell will have everything specified in the ~devShell~ above added to its path. This means we can, for example, use the ~dotnet~ command and we will use the version specified in ~flake.nix~ even if we haven't installed a system wide version of dotnet at all.

*** The F# project
:PROPERTIES:
:ID:       C1D4E2FC-16BA-4BE7-A989-C7A18E7C31B4
:END:

There's absolutely nothing special about this at all. I just created an F# project with ~dotnet~ on the command line, moved ~Program.fs~ into a sub directory called ~src~ because I prefer it that way, and then added a package dependency on ~Falco~ using ~dotnet add package Falco~.

Replace the contents of ~Program.fs~ with:

#+begin_src fsharp
  module Mavnn.CalDance.Server

  open Falco
  open Falco.Routing
  open Falco.HostBuilder

  webHost [||] {
      endpoints [
          get "/" (Response.ofPlainText "Hello World")
      ]
  }
#+end_src

*** Set up the CI pipeline
:PROPERTIES:
:ID:       0960C970-588D-42E9-9D36-2147AD3389BF
:END:

Having used Nix for our development environment, our CI pipeline becomes exceedingly straight forward. All we need is a build container with Nix available and we have all the other information we need for the build already. Nix themselves provide a ~nixos/nix~ image (Nix is the package manager, NixOS is the linux distribution that uses Nix as its package manager) so we'll just use that.

There's a little bit of boilerplate to tell nix that we want to allow flakes and to allow connection to the gitlab package registry. Once that is done, we log into the registry for this project using the CI provided environment variables, run ~nix build .#dockerImage~ and then push the results up to the registry.

#+begin_src yaml
  build-container:
    image:
      name: "nixos/nix:2.19.3"
    variables:
      IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
    before_script:
      - nix-env --install --attr nixpkgs.skopeo
    script:
      - mkdir -p "$HOME/.config/nix"
      - echo 'experimental-features = nix-command flakes' > "$HOME/.config/nix/nix.conf"
      - mkdir -p "/etc/containers/"
      - echo '{"default":[{"type":"insecureAcceptAnything"}]}' > /etc/containers/policy.json
      - skopeo login --username "$CI_REGISTRY_USER" --password "$CI_REGISTRY_PASSWORD" "$CI_REGISTRY"
      - 'nix build .#dockerImage'
      - ls -lh ./result
      - 'skopeo inspect docker-archive://$(readlink -f ./result)'
      - 'skopeo copy docker-archive://$(readlink -f ./result) docker://$IMAGE_TAG'
#+end_src

It's worth noting here that Nix is a deterministic build system (for example, stripping dates from compiled metadata so building the same source code on a different day doesn't product a different binary). In a "real life" context I would be caching the results of the nix build steps to a service like [[https://www.cachix.org/][Cachix]] so that they could be reused between builds, which becomes increasingly useful as the project grows and starts to be comprised of multiple build steps (Nix will be able to cache each "step" individually, even if you only ask for the final outcome of the process).

*** Wrapping it all up
:PROPERTIES:
:ID:       96716D81-4412-44F8-8CC7-F5B32474554D
:END:

Not a bad first days work, I'd say. Our project is already at a stage that we can work on it with standard .NET tooling (for instance, adding a new nuget package with ~dotnet package add ...~ will automatically flow through to that package being added to the docker image) and CI will produce on push a lean deployable artifact. Versions of /everything/ we are using from the .NET SDK to the nuget package we're depending on are fixed across all environments, and we have a nice place to add more developer tooling as we move forwards - for example standardizing the version of postgresql that will be used during development and in CI.

As a bonus extra, anybody with nix installed can build and run the project without having to know .NET or have any .NET tooling installed; a very nice feature when you have others depending on your work who might want to run your code locally, but may not have chosen the same tech stack.

*** Feedback? Comments?
:PROPERTIES:
:ID:       B08DC9D5-A6B1-48D6-A196-6753805362C7
:END:

Have questions? Comments? Hate something, love something, know a better way of doing something? Drop an issue on the repository at [[https://gitlab.com/mavnn/caldance][https://gitlab.com/mavnn/caldance]] and let me know. I'll be pointing a tag at the commit referenced by each blog post, so I can always branch off and include your ideas in a future revision!

*** Next
:PROPERTIES:
:ID:       109E9C29-7E57-4318-9DD3-493F4CAD8708
:END:

[[file:../../../2024/02/06/dev-journal-2.org][Part 2]] adds unit tests and consistent formatting to the project.
* Log in, log out: Dev Journal 4 (part 1)
:PROPERTIES:
:RSS_PERMALINK: 2024/03/01/dev_journal_4.html
:PUBDATE: 2024-03-01
:ID:       BA168719-96B2-4816-A066-DAEEB7D59B6E
:END:
#+begin_quote
This post is part of the "Dev Journal" series. [[file:../../../2024/01/31/dev-journal-1.org][Part 1]] contains the series index, while the [[https://gitlab.com/mavnn/caldance/-/commits/DevJournal4?ref_type=tags][DevJournal4]] tag for the CalDance project in GitLab holds the state of the repository as described here.
#+end_quote

This is the big one: we have our first piece of event sourcing, and a bunch of infrastructure to get us there. So big, in fact, that I'm going to split the post into two and publish the remainder early next week.

A lot has changed, and I'm not going to go into every single detail so if you're following along by hand I made a pull request for the changes added here so that you can [[https://gitlab.com/mavnn/caldance/-/merge_requests/2/diffs][see them all in one place]].

** Nix pulling its weight
:PROPERTIES:
:ID:       4595DB10-A53E-45FA-A826-57ADCB5638BA
:END:

We're about to add a database to our project, and this is an area where Nix really shines.

Adding services with pinned versions of dependencies to are development environment is as simple as adding them to the list in ~flake.nix~:

#+begin_src nix
  devShells.default = pkgs.mkShell {
    buildInputs = [
      dnc.sdk_8_0
      pkgs.nixfmt
      pkgs.skopeo
      pkgs.overmind
      pkgs.tmux
      pkgs.postgresql
      fantomas
      format-all
      format-stdin
      local_postgres
    ];
  };
#+end_src

The only clever thing we're doing here is also adding a ~local_postgres~ command which runs postgres with its data directory set to be a git ignored directory in the repository. This means that a simple git clean will reset the database along with everything else.

As a courtesy to developers who may work on code that isn't CalDance, we also set a non-standard port for postgres to use in our ~.envrc~ file so that we don't compete with any system wide installations that may already be running.

Overmind is a process runner that runs processes as defined in a ~Procfile~, so we add one to the root of the project with the following:

#+begin_src procfile
  server: dotnet watch --project Server/CalDance.Server.fsproj
  postgres: local_postgres
#+end_src

Now we can run ~overmind s~ to start both postgres and a dotnet watcher to live recompile our server code as it changes.

** Adding some nuget dependencies
:PROPERTIES:
:ID:       CA68AAE0-8409-4273-B5DA-3D0F1213E85B
:END:

We're adding dependencies to our server of [[https://martendb.io/][Marten]] (document/event database library that sits on top of postgres) and [[https://serilog.net/][Serilog]] (a nice structured log library).

Marten depends on a postgres library with native (i.e. non-dotnet) dlls, so to allow Nix to cache and link to the correct versions of the native code we have to specify which runtimes we expect to be building our code for. For the curious minded, you don't need to do this to be able to run ~dotnet build~ directly because the ~dotnet~ cli will dynamically download and add the required native libraries - which breaks Nix's caching strategy of a reproducible output from a fixed set of input files.

This isn't a huge issue once you know you need to do it; you just add a ~RuntimeIdentifiers~ node to your project files under the ~TargetFramework~ node like so:

#+begin_src xml
  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net8.0</TargetFramework>
    <RuntimeIdentifiers>osx-arm64;linux-x64;linux-arm64</RuntimeIdentifiers>
  </PropertyGroup>
#+end_src

Then we can add our nuget packages as normal and everything continues to work:

#+begin_src xml
  <ItemGroup>
    <PackageReference Include="Falco" Version="4.0.6" />
    <PackageReference Include="Marten" Version="6.4.1" />
    <PackageReference Include="Serilog" Version="3.1.1" />
    <PackageReference Include="Serilog.AspNetCore" Version="8.0.1" />
    <PackageReference Include="Serilog.Sinks.Console" Version="5.0.1" />
  </ItemGroup>
#+end_src

** Opinionated endpoint builders
:PROPERTIES:
:ID:       AFF9DE1A-5E5B-4D65-8FB2-848E11ACEC05
:END:

In general, the code to handle an endpoint in an AspNet.Core application is a function from ~HttpContext~ to ~Task~, where we mutate the HTTP context and then write the correct output stream.

Falco gives us an abstraction a little higher than that by giving us a set of composable functions for manipulating the HTTP context, which is already a step forward. But I was finding them harder to compose than I would like because in several cases the functions took two inputs and effectively "branched" the response that could be given - for example, do I have the form fields I expect in this POST request, or am I logged in.

I quickly realized that I'd be happier with some kind of "result" mechanism - a way to be able to declare during the specification of a handler that I wanted to short circuit from this point onwards with a failure response.

I also knew that I wanted a type safe way of writing handlers for paths with "place holder" sections.

Because of that, I added a ~Routing~ module in which I've defined a ~Handler~ type as below:

#+begin_src fsharp
  type Handler<'a> =
    HttpContext -> Task<HttpContext * Result<'a, HttpHandler>>
#+end_src

For the sharp eyed among you with functional programming experience you may have spotted this is the same shape as the monad type of a stateful either monad, and indeed we also define a computational expression called ~handler~ that allows us to now write our handlers in a more declarative style.

The revised ~indexEndpoint~ in the main program file gives a good example of what it looks like:

#+begin_src fsharp
  let indexRoute = literalSection "/"

  let indexEndpoint =
    Handler.toEndpoint get indexRoute (fun () ->
      handler {
        let! user = User.getSessionUser

        return
          (Response.ofHtml (
            Elem.html
              []
              [ Elem.body
                  []
                  [ Elem.h1
                      []
                      [ match user with
                        | Some u ->
                          Text.raw $"Hi {u.username}!"
                        | None ->
                          Text.raw "You should go log in!" ]
                    Elem.p
                      []
                      [ Text.raw "Would you like to "
                        Elem.a
                          [ Attr.href (
                              greetingRoute.link "Bob"
                            ) ]
                          [ Text.raw "greet Bob?" ] ] ] ]
          ))
      })
#+end_src

Note the ~let!~ on the first line where we pull the user session out of the HTTP context which the computational expression is "invisibly" carrying along for us.

** Connecting up the database
:PROPERTIES:
:ID:       CAE45CD9-B915-48E8-9E00-274295EDBACE
:END:

Having defined our handler type, it makes sense to make the rest of our tooling easy to use from within the abstraction.

The new ~Marten~ module contains some boiler plate to configure Marten and add Serilog logging to it, but most importantly it also adds:

#+begin_src fsharp
  let withMarten f =
    Handler.fromCtx (fun ctx ->
      ctx.GetService<IDocumentSession>())
    |> Handler.bind (f >> Handler.returnTask)

  // Marten returns null if a record isn't found, but
  // F# records declare they can't be null. This works
  // around that to return an option instead
  let returnOption v =
    if (v |> box |> isNull) then None else Some v
#+end_src

Now from within any HTTP handler we're writing, we can write code like:

#+begin_src fsharp
  let! user =
    Marten.withMarten (fun marten ->
      marten.LoadAsync<UserRecord>(id))
#+end_src

...and as if by magic the request specific Marten session will be pulled out of the HTTP context of the request and we can use it to connect to our data source.

** To be continued...
:PROPERTIES:
:ID:       701D89AC-8329-4CD2-AB90-3FA663EC9AE7
:END:

I think that's about enough for this blog post, because I want to leave a whole post for the real meat of this set of changes: our first domain entity, the ~User~.

If you want a sneak peak, you can check out the PR and see how we can define a neat vertical slice of responsibility in our code base. The module takes the responsibility for user management all the way from the domain object, the events that can happen to it, the Marten config to make sure those are tracked, through to the paths that it has responsibility for and the UI that will be displayed when they are requested. Lots of fun stuff for us to talk about in the next exciting installment of "Dev Journal": different time, multiple channels, next week.

Next up: [[file:../../../2024/03/05/dev_journal_4_2.org][Log in, log out (part 2)]]
* Do notation for TypeScript
:PROPERTIES:
:RSS_PERMALINK: 2024/02/19/do-notation-for-typescript.html
:PUBDATE: 2024-02-19
:ID:       BF1AC731-E84B-4BB7-A72A-6571C7D45425
:END:
This is rather an aside from recent blog posts, but something I found interesting none the less.

Fair warning to start us off: this post assumes that you are aware of and understand "do notation" (or "computational expressions" or "monad syntax") and like the idea of having it available in TypeScript.

It starts by working through a possible way of implementing a type safe representation of a sequence of monadic operations that has a much nicer user experience than nested continuation functions, and then leads into a lengthy example of both building and showing how to use a monad which I've found very useful when working in TypeScript for handling asynchronous code that needs to meaningfully respond to both successes and failures.

The idea is that we're going to go from code that looks like this:

#+begin_src typescript
  export const processLaxCallback = ({
    laxOperations,
    commands,
    localFunctions,
  }: LaxCallbackDependencies) => async (httpRequest) => {
    try {
      const laxSignatureCheck = await laxOperations.checkSignature(httpRequest)
      if(isFailure(signature)) {
        await reportError(signature)
        return
      }
      const laxContext = laxOperations.parseRequest({ httpRequest, laxSignatureCheck })
      if(isFailure(laxContext)) {
        await reportError(laxContext)
        return
      }
      // ...continued
    } catch (e) {
      // ... etc
    }
  }
#+end_src

...to code that looks more like this:

#+begin_src typescript
  export const processLaxCallback = ({
    laxOperations,
    commands,
    localFunctions,
  }: LaxCallbackDependencies) =>
    SolidChain.start<{ httpRequest: HttpRequest }, LaxCallBackState>()
      .chain("laxSignatureCheck", laxOperations.checkSignature)
      .chain("laxContext", laxOperations.parseRequest)
      // ...continued
#+end_src

If you're impatient you can jump straight to [[id:8B8152C2-E896-4933-A30E-E01276B284A8][appendix 2]] where you will find a cut and pastable code block with everything you need to play with the code in the TypeScript editor of your choice.

For the avoidance of any doubt, all the code in this blog post is available for re-use under the MIT license as list in [[id:E8C7C73E-C564-4CDE-B2D9-328AFDF256F1][appendix 3]].

** The idea
:PROPERTIES:
:ID:       651D27A6-B0D6-4B7D-B815-3659CC0E22B9
:END:

TypeScript has one form of monad notation already - the ~await~ keyword. Unfortunately, there isn't any way to plug into the mechanism used and define your own alternative ~bind~ implementation without doing something dangerously hacky. And, frankly, the last thing your TypeScript code needs is an other sharp edge to cut yourself on.

But... what does binding a value in monad notation really do? It doesn't allow you to write code you couldn't have written anyway long hand. It allows you to give the result of a calculation in your code in name in the current scope.

So: if we consider the fact that a scope is really just a mapping from names to values, and that TypeScript allows function inputs to alter the type of their output... maybe we can do something with that?

** Defining a scope
:PROPERTIES:
:ID:       8CC4508E-F9E8-4689-93EF-1D90E9C98BBF
:END:

A type that maps names to values is reasonably easy to define in TypeScript. It looks something like this:

#+begin_src typescript
  export type Scope<Keys extends string> = {
    [K in Keys]: any;
  };
#+end_src

We can say that anything we're willing to consider as a scope is a type that extends the type above: it will have some keys, which will all be strings, and they will map to some values, which will all be sub types of ~any~.

Now we need a type safe way to add a value to the scope.

We start with a calculated type which works out what the result of adding a value with a name to a scope should be:

#+begin_src typescript
  export type ExtendedScope<
    OldScope extends Scope<any>,
    NewField extends string,
    NewValue
  > = OldScope extends any
    ? {
        [K in keyof OldScope | NewField]: K extends NewField
          ? NewValue
          : K extends keyof OldScope
          ? OldScope[K]
          : never;
      }
    : never;
#+end_src

