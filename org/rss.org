#+TITLE: Mavnn's blog

* ADHD and TDD
:PROPERTIES:
:RSS_PERMALINK: 2024/02/21/adhd_and_tdd.html
:PUBDATE: 2024-02-21
:ID:       E1DE423E-5C15-431A-8414-AE7823DA2012
:END:
#+begin_quote
This piece of writing started as repost comment on LinkedIn responding to a question from J. B. Rainsberger about [[https://www.linkedin.com/posts/jbrains_tdd-adhd-adhd-activity-7165713710492176385-xK56][how people with an ADHD diagnosis experienced TDD]]. It's a good question, but it's the kind of topic I "own" enough that I don't want to leave my thoughts on it locked away on a platform like LinkedIn, so now they're here too!
#+end_quote

This is an interesting one for me; I have both an ADHD diagnosis and I do feel that I use certain programming practices to compensate for it. Something that interests me enough that I'm talking on the subject at Lambda Days 2024, in fact.

Any practice that starts with building a feed back loop is going to be helpful from the ADHD point of view; for example, I have a strong preference for strongly typed languages because the compiler will remind me to finish bits I've forgotten about or catch the typos of concentration lapses in the wild.

That said, I'm personally not a huge fan of TDD as I've normally met it in the wild (I can see the one true Scotsman replies from here, and some of you are probably even right). And the reason for that dislike is also ADHD related but has nothing to do with the initial writing of the code - but with refactoring.

In the vast majority of cases I've seen, TDD has led to code bases with many, many, tests that are tightly coupled to the current implementation of a piece of code because what's actually happened is that each test has been written to confirm the next piece of the implementation is doing what is expected, not to test the overall inputs/outputs of the "block". Mocks that check they are called in a certain order or with certain inputs. Carefully crafted fake dependencies which return the data needed in the correct order and type to satisfy the internal of the function being tested. (Is this what TDD is meant to be? No, not at all - I'm aware)

This makes refactoring deeply painful with a pain that hits hard at the heart of ADHD - you're faced with the choice of updating all these painful, pointless, implementation internal specific mocks on the one hand or having the argument about why your refactoring PR "reduces test coverage" on the other. If you're particularly unlucky there aren't any tests covering the actual _behaviour_ of the original code at all, and now you're left wondering:

"But is my refactor really a refactor at all?"

(It would be remiss of me not to note that teaching developers how to test behaviour and not implementations is actually a service I offer, partly because I don't want to live with the consequences of them not knowing! See [[file:../../../2024/01/29/short_term_help.org][my short term consulting page]].)
* Does it run? Dev Journal 3
:PROPERTIES:
:RSS_PERMALINK: 2024/02/20/dev-journal-3.html
:PUBDATE: 2024-02-20
:ID:       953B1D6E-8F88-44A7-94A1-C9CE6DF2AB93
:END:
#+begin_quote
This post is part of the "Dev Journal" series. [[file:../../../2024/01/31/dev-journal-1.org][Part 1]] contains the series index, while the [[https://gitlab.com/mavnn/caldance/-/commits/DevJournal3?ref_type=tags][DevJournal3]] tag for the CalDance project in GitLab holds the state of the repository as described here.
#+end_quote

A short update this time. [[https://gitlab.com/mavnn/caldance/-/issues/3][Gregg Bremer]] (hi Gregg!) pointed out that running ~nix run~ on his linux machine resulted in an error about not being able to find ~libssl~.

This neatly highlights one of the weak spots of Nix; while an excellent packaging solution, it isn't perfect. Nix sandboxes your packages by altering the path environment variable, but not everything is located via that mechanism.

In this case, ~nix run~ ran on my machine because I happened to have the libraries in the "right place" for a self contained dotnet core executable, but Gregg did not.

Neither, it turns out, did the docker container I was building. I built and tested it initially with a dotnet console app (which did work, not needing ~libssl~) and then carried on assuming that running ~nix run~ on my local machine would also tell me if the docker image could run correctly.

I've now fixed up the code in the previous posts (we needed to add some ~runtimeDeps~ to our server package, and the docker image start up command needs to create a writable ~/tmp~ directory for asp.net to run correctly).

Most importantly though, I've also made sure that CI will prevent this from happening again by actually checking that the docker image produced can respond to a request to the index with a 200 response code. This is done by adding "stages" to our CI build; the first does exactly what we were doing already, the second then starts the just finished docker image as a "service" and uses ~curl~ to check it can respond to us.

You can check out the revised ~.gitlab-ci.yml~ file below:

#+begin_src yaml
  stages:
    - build-container
    - end-to-end-tests

  build-container:
    stage: build-container
    image:
      name: "nixos/nix:2.19.3"
    variables:
      IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
    before_script:
      - nix-env --install --attr nixpkgs.skopeo
    script:
      - mkdir -p "$HOME/.config/nix"
      - echo 'experimental-features = nix-command flakes' > "$HOME/.config/nix/nix.conf"
      - mkdir -p "/etc/containers/"
      - echo '{"default":[{"type":"insecureAcceptAnything"}]}' > /etc/containers/policy.json
      - skopeo login --username "$CI_REGISTRY_USER" --password "$CI_REGISTRY_PASSWORD" "$CI_REGISTRY"
      - 'nix build .#dockerImage .#test'
      - mkdir testResults
      - 'cp result-1/* testResults'
      - ls -lh ./result
      - 'skopeo inspect docker-archive://$(readlink -f ./result)'
      - 'skopeo copy docker-archive://$(readlink -f ./result) docker://$IMAGE_TAG'
    artifacts:
      when: always
      paths:
        - 'testResults/*.xml'
      reports:
        junit: 'testResults/*.xml'

  end-to-end-tests:
    stage: end-to-end-tests
    image:
      name: "nixos/nix:2.19.3"
    variables:
      IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
      GIT_STRATEGY: none
    services:
      - name: $IMAGE_TAG
        alias: caldance
    script:
      - curl -f "http://caldance:5001/"
#+end_src
* Do notation for TypeScript
:PROPERTIES:
:RSS_PERMALINK: 2024/02/19/do-notation-for-typescript.html
:PUBDATE: 2024-02-19
:ID:       5F548920-8B57-4D5F-8D19-EBD67A47444E
:END:
This is rather an aside from recent blog posts, but something I found interesting none the less.

Fair warning to start us off: this post assumes that you are aware of and understand "do notation" (or "computational expressions" or "monad syntax") and like the idea of having it available in TypeScript.

It starts by working through a possible way of implementing a type safe representation of a sequence of monadic operations that has a much nicer user experience than nested continuation functions, and then leads into a lengthy example of both building and showing how to use a monad which I've found very useful when working in TypeScript for handling asynchronous code that needs to meaningfully respond to both successes and failures.

The idea is that we're going to go from code that looks like this:

#+begin_src typescript
  export const processLaxCallback = ({
    laxOperations,
    commands,
    localFunctions,
  }: LaxCallbackDependencies) => async (httpRequest) => {
    try {
      const laxSignatureCheck = await laxOperations.checkSignature(httpRequest)
      if(isFailure(signature)) {
        await reportError(signature)
        return
      }
      const laxContext = laxOperations.parseRequest({ httpRequest, laxSignatureCheck })
      if(isFailure(laxContext)) {
        await reportError(laxContext)
        return
      }
      // ...continued
    } catch (e) {
      // ... etc
    }
  }
#+end_src

...to code that looks more like this:

#+begin_src typescript
  export const processLaxCallback = ({
    laxOperations,
    commands,
    localFunctions,
  }: LaxCallbackDependencies) =>
    SolidChain.start<{ httpRequest: HttpRequest }, LaxCallBackState>()
      .chain("laxSignatureCheck", laxOperations.checkSignature)
      .chain("laxContext", laxOperations.parseRequest)
      // ...continued
#+end_src

If you're impatient you can jump straight to [[id:8B8152C2-E896-4933-A30E-E01276B284A8][appendix 2]] where you will find a cut and pastable code block with everything you need to play with the code in the TypeScript editor of your choice.

For the avoidance of any doubt, all the code in this blog post is available for re-use under the MIT license as list in [[id:E8C7C73E-C564-4CDE-B2D9-328AFDF256F1][appendix 3]].

** The idea
:PROPERTIES:
:ID:       FDD7CF41-5239-4556-8BF9-A11AF4E70FF5
:END:

TypeScript has one form of monad notation already - the ~await~ keyword. Unfortunately, there isn't any way to plug into the mechanism used and define your own alternative ~bind~ implementation without doing something dangerously hacky. And, frankly, the last thing your TypeScript code needs is an other sharp edge to cut yourself on.

But... what does binding a value in monad notation really do? It doesn't allow you to write code you couldn't have written anyway long hand. It allows you to give the result of a calculation in your code in name in the current scope.

So: if we consider the fact that a scope is really just a mapping from names to values, and that TypeScript allows function inputs to alter the type of their output... maybe we can do something with that?

** Defining a scope
:PROPERTIES:
:ID:       30CBCF33-E2DC-4638-817F-D8983EA9214B
:END:

A type that maps names to values is reasonably easy to define in TypeScript. It looks something like this:

#+begin_src typescript
  export type Scope<Keys extends string> = {
    [K in Keys]: any;
  };
#+end_src

We can say that anything we're willing to consider as a scope is a type that extends the type above: it will have some keys, which will all be strings, and they will map to some values, which will all be sub types of ~any~.

Now we need a type safe way to add a value to the scope.

We start with a calculated type which works out what the result of adding a value with a name to a scope should be:

#+begin_src typescript
  export type ExtendedScope<
    OldScope extends Scope<any>,
    NewField extends string,
    NewValue
  > = OldScope extends any
    ? {
        [K in keyof OldScope | NewField]: K extends NewField
          ? NewValue
          : K extends keyof OldScope
          ? OldScope[K]
          : never;
      }
    : never;
#+end_src

